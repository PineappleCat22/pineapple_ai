hey so idk how to access cloned models so this just uses ollama.
i need to figure out how to use modelfiles which point to the model location, i guess.
to do, find out where ollama stores model files.
some notes:
using base llama 2 uncen, it doesnt seem to acknowledge itself sometimes. i told it that i was its creator and it was just... fine with that.
not really what im shooting for. if i can create some kind of handler rule so that it can handle stuff like that, would be schweet
want to try using the ai with and without the bge embedding model.

as of now im still waiting on the discord data to comb through, format for ai use, and uhhh yknow install it
that will pose its own problems, but for now humanizing the ai and preparing it to interface with things like discord and shit is a good first step.
i am a little concerned about how some of this poses ai models as a data retrieval question (ie asking a question and having an answer in a dataset)
this does pose a problem: how does me-ai handle questions? and questions without answers? planning on including a basic file of facts for the ai to pull from in that case
but for questions without answers, i will need to tune the ai's ability to get creative.

also hey im using a python venv for this so heres where the road gets bumpy.
for some reason the environment is externally managed
i was helped along by this thread: https://askubuntu.com/questions/1465218/pip-error-on-ubuntu-externally-managed-environment-%C3%97-this-environment-is-extern
ill keep a comprehensive list here of packages installed in the virtual environment.
llama-index-llms-ollama
llama-index-embeddings-huggingface

uhhhh i dont know if you need to like... source to the venv every time?
heres the command you run to point to the venv:
source .venv/bin/activate
i hope it works how i think it does so i can automate it

addendum to the first three lines of notes:
I FIGURED IT OOOOOOOOOOOOOOOOOOUT
its pointless but for the sake of portability i will create the modelfiles for both bge and ll2-u, and leave them in.

yeah i didnt do that.
hey buddy you're probably wondering what that venv is doing there. the interpretation running on my laptop needs it for some damn reason. god this is so janky but if it works it works.
pep8 is a fucking bitch.
intellij is a fucking bitch.
STOP POLICING MY LANGUAGE
THE VENV IS GONE
this is my third time installing the packages on my laptop side. FUCK!
the venv is entirely missing and i cant get it back. so im using my laptop's environment
the venv is back
unresolved reference: print
WHAT
WHAT DO YOU MEAN SDK is not defined for Run Configuration
IT WONT WORK
WHAT THE FUCK
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
it works

created new subdir for dave to live in.
shouldve said a while ago, but i switched to llama 3
i also made a devlog im not adding dates tho fuck that shit
THE VENV ON THE PI KEEPS FORGETTING MY MODULES?????????????????????????????

ok as it turns out for what im trying to do, llama index fucking sucks.
i need to find something to pivot to. going to ask chatgpt but i have expended all available braincells for tonight.
so TODO: kill llamaindex, pick something else to serve the ai.
from what chatgpt is spitting to me right now (straight immutable facts) i will need a local model. ollama can still be used for this.
ollama =/= llamaindex, and might still be of some use to me. i weep however.

good morning. a new truckload of braincells arrived today.
while i was napping i had a thought about llamaindex.
the reason it doesnt work is because i want to fine-tune an ai based on my discord messages, and that doesn't give me that ability.
llamaindex is built more for Q&A response, however that might still be of use to me.
the original plan was to fine tune the model and then hand it a file full of "facts".
this would exisit in the event that someone asks a question about me so that the ai isnt making up details of my personal life.
that approach might still work with llamaindex, but theres other problems too.
in previous test attempts, it seems to ignore me. even when i try to run it like a chat bot it REALLY wants to answer questions.
in this pursuit it ignores me and asks itself a question. this is clearly not intended behavior
if i can control this the llamaindex ai might be just what i need. wont work for dave though.

i have been playing roblox for two days
time to buckle down and get SOMETHING done
hey. my intellij isnt working. wowza
HUMANITY RESTORED time to begin the refactor
i CANT FUCKING ACCESS LLAMA3 BECAUSE ITS GATED BEHIND A FUCKING ACCESS REQUEST
i thought meta were chill but ig they only are to ollama cuz you can use it on that without even looking at their terms of use
FUCK!!!
in theory everything has been set up and prepared for use with pytorch. hopefully these tests go better.
if this works, we can go on to step two: training. hehehehaw
THAT WAS QUICK.
llama 3 is fucking huge. and needs to be lfs'd