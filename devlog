hey so idk how to access cloned models so this just uses ollama.
i need to figure out how to use modelfiles which point to the model location, i guess.
to do, find out where ollama stores model files.
some notes:
using base llama 2 uncen, it doesnt seem to acknowledge itself sometimes. i told it that i was its creator and it was just... fine with that.
not really what im shooting for. if i can create some kind of handler rule so that it can handle stuff like that, would be schweet
want to try using the ai with and without the bge embedding model.

as of now im still waiting on the discord data to comb through, format for ai use, and uhhh yknow install it
that will pose its own problems, but for now humanizing the ai and preparing it to interface with things like discord and shit is a good first step.
i am a little concerned about how some of this poses ai models as a data retrieval question (ie asking a question and having an answer in a dataset)
this does pose a problem: how does me-ai handle questions? and questions without answers? planning on including a basic file of facts for the ai to pull from in that case
but for questions without answers, i will need to tune the ai's ability to get creative.

also hey im using a python venv for this so heres where the road gets bumpy.
for some reason the environment is externally managed
i was helped along by this thread: https://askubuntu.com/questions/1465218/pip-error-on-ubuntu-externally-managed-environment-%C3%97-this-environment-is-extern
ill keep a comprehensive list here of packages installed in the virtual environment.
llama-index-llms-ollama
llama-index-embeddings-huggingface

uhhhh i dont know if you need to like... source to the venv every time?
heres the command you run to point to the venv:
source .venv/bin/activate
i hope it works how i think it does so i can automate it

addendum to the first three lines of notes:
I FIGURED IT OOOOOOOOOOOOOOOOOOUT
its pointless but for the sake of portability i will create the modelfiles for both bge and ll2-u, and leave them in.

yeah i didnt do that.
hey buddy you're probably wondering what that venv is doing there. the interpretation running on my laptop needs it for some damn reason. god this is so janky but if it works it works.
pep8 is a fucking bitch.
intellij is a fucking bitch.
STOP POLICING MY LANGUAGE
THE VENV IS GONE
this is my third time installing the packages on my laptop side. FUCK!
the venv is entirely missing and i cant get it back. so im using my laptop's environment
the venv is back
unresolved reference: print
WHAT
WHAT DO YOU MEAN SDK is not defined for Run Configuration
IT WONT WORK
WHAT THE FUCK
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
it works

created new subdir for dave to live in.
shouldve said a while ago, but i switched to llama 3
i also made a devlog im not adding dates tho fuck that shit
THE VENV ON THE PI KEEPS FORGETTING MY MODULES?????????????????????????????

ok as it turns out for what im trying to do, llama index fucking sucks.
i need to find something to pivot to. going to ask chatgpt but i have expended all available braincells for tonight.
so TODO: kill llamaindex, pick something else to serve the ai.
from what chatgpt is spitting to me right now (straight immutable facts) i will need a local model. ollama can still be used for this.
ollama =/= llamaindex, and might still be of some use to me. i weep however.

good morning. a new truckload of braincells arrived today.
while i was napping i had a thought about llamaindex.
the reason it doesnt work is because i want to fine-tune an ai based on my discord messages, and that doesn't give me that ability.
llamaindex is built more for Q&A response, however that might still be of use to me.
the original plan was to fine tune the model and then hand it a file full of "facts".
this would exisit in the event that someone asks a question about me so that the ai isnt making up details of my personal life.
that approach might still work with llamaindex, but theres other problems too.
in previous test attempts, it seems to ignore me. even when i try to run it like a chat bot it REALLY wants to answer questions.
in this pursuit it ignores me and asks itself a question. this is clearly not intended behavior
if i can control this the llamaindex ai might be just what i need. wont work for dave though.

i have been playing roblox for two days
time to buckle down and get SOMETHING done
hey. my intellij isnt working. wowza
HUMANITY RESTORED time to begin the refactor
i CANT FUCKING ACCESS LLAMA3 BECAUSE ITS GATED BEHIND A FUCKING ACCESS REQUEST
i thought meta were chill but ig they only are to ollama cuz you can use it on that without even looking at their terms of use
FUCK!!!
in theory everything has been set up and prepared for use with pytorch. hopefully these tests go better.
if this works, we can go on to step two: training. hehehehaw
THAT WAS QUICK.
llama 3 is fucking huge. and needs to be lfs'd
llama is installed time to do the silly shit with it
first test nearly crashed the laptop. im going to need a more powerful test environment...
to do list: buy ram for the workstation.
we've reached another roadblock: llama 3 is a ram hog. once my next paycheck comes in i plan on buying 4x 16gb ddr3 ram and 1x phanteks xt pro case for my main pc.
shutting down for the night.

payday came early. the money has been transferred and sent, my shit comes in next week.

killed time a little too hard. nothing i can do atm but start the bot framework, so i do that.
i should also containerize the chatbot code because it shouldnt be in start.py
ill do that first, since its easy.
i will say working on two different ais with two different use cases is pretty beneficial to the progress of both.
i threw dave in the start.py and created some handlers so that you can import it and use it to run meta, pineapple, or dave.
im going to rename it too so for brevity start.py is now runModel.py.
start.py will now contain the code to spin up the discord bot and some basic responses.
just to get the hang of the thing ill make it say "hi!" when you ping it, and a command to add two numbers or some shit
i might need to create some kind of prompt queue? so it doesnt try running the query code too many times at once.
thats another interesting thing. im not sure if the ai will add to its prompt and response history between messages.
theres a lot of unknown here and i cant test it until the RAM gets here. i might also need to write an OoM handler.
FUCK! ok lets get our ducks in a row
problems:
prompt queue? idk if the bot runs the code every time or until it is ready.
need a way to signify that the bot is thinking. this should be ez.
chat history is a concern as different servers shouldnt have the same history but the bot should still remember messages.
for now we have a framework. push and commit
as of now the /dave/ directory is empty save for the dave data and the explanation.
im trashing the explanation and the directory will contain the model files for dave in the future.

just got back. dont think im awake enough to finish the work here.
discord.py's documentation is really ass. theres a lot of things nested in other things, not enough code snippets, and not enough succinct descriptions.
i dont want to chatgpt it but im being pushed here. fuck you discord.
i dont want to spend hours reading docs just to figure out how to capture an input message.
gonna test slash commands by making something to add two numbers. basic stuff. no idea how to do it though.
the bot has a tendency to get rate limited when you harass it too much. dont bully him, okay?
still not sure how sharding works and all that. need to test. fuk. retiring my brain for now. gn

little bit of extra work. mostly just flavor stuff and im also learning how to handle messages (to pass them on to the ai)

still just milling around adding random shit.
i added some new stuff to the pineapple system prompt
start.py now has a testmode to switch between normal operation and debug. is this safe? probably not
stole tokenizer.py to test tokens with testmode.

the ram finally arrived, so thats cool. im realizing that i got cheap ram and maybe it was a bad idea.
also the bios gave a warning that the psu wouldnt support it.
but we're 42% through the memtest with no fails, so thats cool. this fucker is chugging along. it could be the ecc modules putting in work or something
75 degrees c but still moving fuck yeah
it hit 80. AND ITS STILL WORKING BABYEEE. SOUNDS LIKE A JET ENGINE
i wrote a message handler def to automatically flag >2000 messages, and also tried to reduce the number of awaits.

...so uh, the ai is taking forever. and while waiting i found this: https://github.com/ollama/ollama-python
which was exactly what i was looking for when i used transformers
god fuckign damn it
THE TRANSFORMERS AI FRAME WORK IS FUCKING BRAIN DEAD. IT SCREAMED AT ME AND DIED. COCK
im saving all the old code in runModelOLD.py because im too stubborn to delete it all. fuck

its been a while. i have it pretty much settled now.
only thing i need to do is adjust some system prompt stuff and try and get more response time.
bot end is pretty much done.
added some friends to bot test hell to help test the ai, nom gaslit it and seba immediately prompt engineered it
theres some relevant TODOs in start.py to help adjust its personality, ill get to those later.
i might be able to increase its creativity by adding more temperature? i need to experiment with that.

ai sets its own status now. cool. everything is set up and i have a training script that maybe works?
came to a halt in the middle of installing ROCm so that i could train it on my AMD gpus.
getting an error: cannot find name for group ID 1000
gonna kick back for now. no idea how to fix that.
ai model updated to v1.4, temperature set to 50, minor changes to the system prompt.
TECHNICALLY a temperature of 50 shouldnt be logical, but it seems to work for now, aside from the ai making up its own slang occasionally.
whatever.

added testmode 2 so i can view ai response time.

note to self: write exit condition.
trying to refresh status was a pain in the ass and didnt work. heres the code. try again some other time, sport.
class MyClient(discord.Client):
    try:
        async def on_ready(self):
            #technically you shouldnt do any of this here
            async def set_status():
                i = 1
                while True: #so fucking cringe...
                    def query_ai_for_status(): # DEFS INSIDE DEFS INSIDE DEFS INSIDE DEFS INSIDE DEFS INSIDE DEFS INSIDE DEFS INSIDE DEFS INSIDE DEFS INSIDE DEFS INSIDE ...
                        #return model.query("Describe what you are doing in ten words or less.", modelName)
                        return "balls"
                    loop = asyncio.get_event_loop()
                    status_message = await loop.run_in_executor(executor, query_ai_for_status)
                    await self.change_presence(status=discord.Status.idle, activity=discord.CustomActivity(status_message))
                    await asyncio.sleep(60)
                    print(f"status changed {i} times")
                    i += 1
            asyncio.run(set_status()) #this is fucking goofy.
            print(f'{self.user} initialized successfully')
            print("test mode:", testmode)

note to self: write exit condition.
trying to refresh status was a pain in the ass and didnt work. heres the code. try again some other time, sport.
class MyClient(discord.Client):
    try:
        async def on_ready(self):
            #technically you shouldnt do any of this here
            async def set_status():
                i = 1
                while True: #so fucking cringe...
                    def query_ai_for_status(): # DEFS INSIDE DEFS INSIDE DEFS INSIDE DEFS INSIDE DEFS INSIDE DEFS INSIDE DEFS INSIDE DEFS INSIDE DEFS INSIDE DEFS INSIDE ...
                        #return model.query("Describe what you are doing in ten words or less.", modelName)
                        return "balls"
                    loop = asyncio.get_event_loop()
                    status_message = await loop.run_in_executor(executor, query_ai_for_status)
                    await self.change_presence(status=discord.Status.idle, activity=discord.CustomActivity(status_message))
                    await asyncio.sleep(60)
                    print(f"status changed {i} times")
                    i += 1
            asyncio.run(set_status()) #this is fucking goofy.
            print(f'{self.user} initialized successfully')
            print("test mode:", testmode)